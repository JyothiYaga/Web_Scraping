{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a96e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 10 minutes...\n",
      "Waiting 10 minutes...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m time_wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Increase wait time to 10 minutes to avoid overloading the server\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWaiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_wait\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_wait\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def scrape_sport_news():\n",
    "    url = 'https://www.bbc.com/sport'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the page was retrieved successfully\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page, status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    li_classes = [\n",
    "        'ssrcss-rs7w2i-ListItem e1gp961v0',\n",
    "        'ssrcss-1dr5icq-ListItem e1gp961v0',\n",
    "        'ssrcss-1r7nvnf-ListItem e1gp961v0',\n",
    "        'ssrcss-13h7haz-ListItem e1gp961v0',\n",
    "        'ssrcss-1amy2cn-ListItem e1gp961v0',\n",
    "        'ssrcss-vmhflp-ListItem e1gp961v0',\n",
    "        'ssrcss-8k5g2t-ListItem e1gp961v0',\n",
    "        'ssrcss-2al3ka-ListItem e1gp961v0',\n",
    "        'ssrcss-8zf5rb-ListItem e1gp961v0',\n",
    "        'ssrcss-kftbws-ListItem e1gp961v0',\n",
    "        'ssrcss-1t3edyj-ListItem e1gp961v0',\n",
    "        'ssrcss-1wfzqxg-ListItem e1gp961v0'\n",
    "    ]\n",
    "\n",
    "    base_url = 'https://www.bbc.com'\n",
    "\n",
    "    news_data = []\n",
    "    serial_number_counter = 0\n",
    "\n",
    "    for li_class in li_classes:\n",
    "        news_items = soup.find_all('li', class_=li_class)\n",
    "\n",
    "        for sport in news_items:\n",
    "            serial_number_counter += 1\n",
    "\n",
    "            news_sports = sport.find('span', class_='ssrcss-1if1g9v-MetadataText e4wm5bw1')\n",
    "            news_sports = news_sports.text if news_sports else \"N/A\"\n",
    "\n",
    "            news_headline = sport.find('p', class_='ssrcss-6arcww-PromoHeadline exn3ah96')\n",
    "            news_headline = news_headline.span.text if news_headline and news_headline.span else \"N/A\"\n",
    "      \n",
    "            content_container = sport.find('p', class_='ssrcss-1q0x1qg-Paragraph e1jhz7w10')\n",
    "            news_content = content_container.text if content_container else \"N/A\"\n",
    "\n",
    "            link_container = sport.find('div', class_='ssrcss-1f3bvyz-Stack e1y4nx260')\n",
    "            news_link = base_url + link_container.a['href'] if link_container and link_container.a else \"N/A\"\n",
    "\n",
    "            img_container = sport.find('span', class_='ssrcss-11kpz0x-Placeholder etlorgc0')\n",
    "            image_url = img_container.find('img')['src'] if img_container and img_container.find('img') else \"N/A\"\n",
    "\n",
    "            news_data.append({\n",
    "                'Serial Number': serial_number_counter,\n",
    "                'Sports': news_sports,\n",
    "                'Headline': news_headline,\n",
    "                'Content': news_content,\n",
    "                'Link': news_link,\n",
    "                'Image URL': image_url\n",
    "            })\n",
    "\n",
    "    return news_data\n",
    "\n",
    "def write_to_csv(news_data):\n",
    "    with open('bbcsports_news.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Serial Number', 'Sports', 'Headline', 'Content', 'Link', 'Image URL']\n",
    "        csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(news_data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while True:\n",
    "        sports_news_data = scrape_sport_news()\n",
    "        if sports_news_data:\n",
    "            write_to_csv(sports_news_data)\n",
    "        \n",
    "        time_wait = 10  # Increase wait time to 10 minutes to avoid overloading the server\n",
    "        print(f'Waiting {time_wait} minutes...')\n",
    "        time.sleep(time_wait * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290a659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
